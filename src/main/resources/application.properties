spring.application.name=Spring AI OpenAI Basics
# Server configuration
server.port=8080

# Allow bean definition overriding
spring.main.allow-bean-definition-overriding=true

# OpenAI API configuration
spring.ai.openai.api-key=${OPENAI_API_KEY}
# Model settings
spring.ai.openai.chat.options.model=gpt-3.5-turbo
spring.ai.openai.chat.options.temperature=0.7
spring.ai.openai.chat.options.maxCompletionTokens=150

# Anthropic Claude API configuration
spring.ai.anthropic.api-key=${ANTHROPIC_API_KEY}
# Model settings
spring.ai.anthropic.chat.options.model=claude-3-5-sonnet-latest
spring.ai.anthropic.chat.options.temperature=0.7
spring.ai.anthropic.chat.options.max-tokens=150

# Ollama Configuration
# Base URL for your local Ollama installation
spring.ai.ollama.base-url=http://localhost:11434
# The model to use - choose from your locally installed models
spring.ai.ollama.chat.options.model=llama3
# Temperature control (0.0 to 1.0) - higher values make output more random
spring.ai.ollama.chat.options.temperature=0.7
# Max tokens to generate in the response
spring.ai.ollama.chat.options.max-tokens=500
# Optional: Enable streaming - set to true if you want to stream responses
# spring.ai.ollama.chat.options.stream=false